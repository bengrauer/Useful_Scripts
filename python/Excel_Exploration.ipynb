{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Ben Grauer \n",
    "# Created a python script that does some basic exploratory analysis / dov and exports to excel for easy viewing\n",
    "# Designed for quick exploratory analysis of the data sets from Kaggle\n",
    "\n",
    "####################\n",
    "# Log\n",
    "####################\n",
    "# 02/00/2017 - Script Created \n",
    "# 04/00/2017 - Added more summary stats, included additional tabs\n",
    "# 05/00/2017 - Re-did stats to include variance, pivoted the data to allow sorting in excel. Added function pullSummaryStats_df \n",
    "# 05/00/2017 - Froze some column headers.  Sample head(75) + tail(75)\n",
    "# 06/00/2017 - created ordered summary stats for quick sorting - also in progress step of re-factoring the code\n",
    "# 07/10/2017 - Fixed the orderd summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Input / Output files\n",
    "\n",
    "# input file to read and analyze \n",
    "inputFileNme = '/project/data/kg_lendingclub/loan.csv'\n",
    "\n",
    "# output file - this can have whatever naming convention you want\n",
    "outputFileName = '/project/data/kg_lendingclub/ANALYSIS_loan.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will pivot the summary stats (with fields in first colmn, and the stats other columns - to sort with)\n",
    "# As time permits - need to clear out the first row - AND then comment out the dfNewStat, since we will not be using it.\n",
    "#   we could have used it, but time-lag and just assigned each variable and then flipped it.  clean up the code\n",
    "def pullSummaryStats_df(df):\n",
    "    \n",
    "    # dfOrdStat - is the fullly ordered stat list - by column\n",
    "    # dfNewStat - is the newly dynamic created set\n",
    "    \n",
    "    # Create a new data frame for the numeric data types\n",
    "    dfNumericTypes = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Set the columns we will use for the data frames\n",
    "    columns = ['count','NaN','NaNPerc','mean','median','std','var','range','0%','25%','50%','75%','100%']\n",
    "    \n",
    "    dfOrdStat = pd.DataFrame(index=(['']), columns = columns )\n",
    "\n",
    "    # for each column, grab stats\n",
    "    for column in dfNumericTypes:\n",
    "                \n",
    "        # count\n",
    "        statCount = np.round(dfNumericTypes[column].count(), 2)\n",
    "        #NaN\n",
    "        NaNCount = len(dfNumericTypes[column]) - dfNumericTypes[column].count()\n",
    "        #NaN%\n",
    "        NaNCountPerc = round(float((len(df) - df[column].count()) / len(df[column])), 4)\n",
    "        # mean\n",
    "        statMean = np.round(dfNumericTypes[column].mean(), 4)\n",
    "        # median\n",
    "        statMedian = dfNumericTypes[column].median()\n",
    "        # Std\n",
    "        statStd = dfNumericTypes[column].std()\n",
    "        # Var\n",
    "        statVar = dfNumericTypes[column].var()\n",
    "        # Range\n",
    "        statRange = dfNumericTypes[column].max() - dfNumericTypes[column].min()\n",
    "        # min\n",
    "        statMin = np.round(float(dfNumericTypes[column].quantile([0.0])), 4) \n",
    "        # 25%\n",
    "        stat25 = np.round(float(dfNumericTypes[column].quantile([0.25])), 4)\n",
    "        # 50%\n",
    "        stat50 = np.round(float(dfNumericTypes[column].quantile([0.50])), 4)\n",
    "        # 75%\n",
    "        stat75 = np.round(float(dfNumericTypes[column].quantile([0.75])), 4)    \n",
    "        # max\n",
    "        statMax = np.round(float(dfNumericTypes[column].quantile([1.0])), 4)\n",
    "        \n",
    "        # Set the data (for the row), index and columns\n",
    "        data = [(statCount, NaNCount, NaNCountPerc, statMean, statMedian, statStd, statVar, statRange, statMin, stat25, stat50, stat75, statMax)]\n",
    "        indexName = ([dfNumericTypes[column].name])\n",
    "        \n",
    "        # 07/10/2017 - Ben Grauer - adjusting here\n",
    "        # Create a new data frame of the data\n",
    "        dfData = pd.DataFrame(data, index=indexName, columns = columns )\n",
    "        \n",
    "        # then concatenate / add on to the data set\n",
    "        dfOrdStat = pd.concat([dfOrdStat, dfData])\n",
    "\n",
    "    # return dfNewStat\n",
    "    return dfOrdStat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to take a workbook, sheetname, and iterate through to populate columns\n",
    "# TODO: Break out by number:  array([dtype('int64'), dtype('float64'), dtype('O')], dtype=object)\n",
    "#      df.dtypes\n",
    "def excel_add_df(inputWorkBook, inputWorkSheet, inputSheetName, inputDataFrame, \n",
    "                 inputStartRow=0,inputStartCol=0, inputUseIndex=False):\n",
    "    \n",
    "    bold = inputWorkBook.add_format({'bold': True})\n",
    "    font14bold = inputWorkBook.add_format({'font_size':14, 'bold': True})\n",
    "    \n",
    "    # local variables\n",
    "    rowNum = inputStartRow\n",
    "    colNum = inputStartCol\n",
    "    \n",
    "    # add the input header about the sheet\n",
    "    inputWorkSheet.write(rowNum, inputStartCol, inputSheetName, font14bold)\n",
    "    rowNum = rowNum + 1\n",
    "    \n",
    "    # add an index\n",
    "    if inputUseIndex==True:\n",
    "        inputWorkSheet.write_column(inputStartRow+3, colNum, inputDataFrame.index, bold)\n",
    "        colNum = colNum + 1\n",
    "    \n",
    "    for column in inputDataFrame:\n",
    "        rowNum = inputStartRow + 2  # for the column headers. so 2 down\n",
    "        inputWorkSheet.write(rowNum, colNum, inputDataFrame[column].name, bold)\n",
    "        \n",
    "        rowNum = inputStartRow + 3\n",
    "        inputWorkSheet.write_column(rowNum, colNum, inputDataFrame[column])\n",
    "    \n",
    "        # add a column\n",
    "        colNum = colNum + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "from pylab import figure, axes, pie, title, show\n",
    "\n",
    "# open the excel sheet\n",
    "def excel_gen_file(df, outputFileName):\n",
    "\n",
    "    #workbook = xlsxwriter.Workbook('LendingClub_DOV.xlsx', {'nan_inf_to_errors': True})\n",
    "    workbook = xlsxwriter.Workbook(outputFileName, {'nan_inf_to_errors': True})\n",
    "\n",
    "    # Add the DOV Work sheet - quite a bit of code\n",
    "    worksheet = workbook.add_worksheet('DOV')\n",
    "\n",
    "    # Add a bold format to use to highlight cells.\n",
    "    bold = workbook.add_format({'bold': True})\n",
    "    italic = workbook.add_format({'italic': True})\n",
    "    underline = workbook.add_format({'underline': True})\n",
    "    formatHigh = workbook.add_format({'font_color': 'red'})\n",
    "    formatLow = workbook.add_format({'font_color': 'blue'})\n",
    "\n",
    "    offSetDataType = 1\n",
    "    offSetNumNaN = 2\n",
    "    offsetCount = 3\n",
    "    offsetMean = 4\n",
    "    offsetMedian = 5\n",
    "    offsetStd = 6\n",
    "    offsetVar = 7\n",
    "    offsetRange = 8\n",
    "    offset0Prct = 9\n",
    "    offset25Prct = 10\n",
    "    offset50Prct = 11\n",
    "    offset75Prct =12\n",
    "    offset100Prct = 13\n",
    "    \n",
    "    offsetContOrDesc = 15\n",
    "    offsetNotes = 16\n",
    "    \n",
    "    offsetRowDataHeader = 18\n",
    "    \n",
    "    offsetRowFreezeRow = 18\n",
    "\n",
    "    # star the column Iteration at 2\n",
    "    colIteration = 1\n",
    "    rowIteration = 0 # to start\n",
    "    rowDataHeader = offsetRowDataHeader # bumpt out to 5 to start, so we have 4 for summary data\n",
    "\n",
    "    # set the row descriptions\n",
    "    worksheet.write(rowIteration, 0, 'Col Name', bold)\n",
    "    worksheet.write(offSetDataType, 0, 'Data Type', bold)\n",
    "    worksheet.write(offSetNumNaN, 0, '# Na', bold)\n",
    "    worksheet.write(offsetCount, 0, 'count', bold)\n",
    "    worksheet.write(offsetMean, 0, 'mean', bold)\n",
    "    worksheet.write(offsetMedian, 0, 'median', bold)\n",
    "    worksheet.write(offsetStd, 0, 'std', bold)\n",
    "    worksheet.write(offsetVar, 0, 'var', bold)\n",
    "    worksheet.write(offsetRange, 0, 'range', bold)\n",
    "    worksheet.write(offset0Prct, 0, '0%', bold)\n",
    "    worksheet.write(offset25Prct, 0, '25%', bold)\n",
    "    worksheet.write(offset50Prct, 0, '50%', bold)\n",
    "    worksheet.write(offset75Prct, 0, '75%', bold)\n",
    "    worksheet.write(offset100Prct, 0, '100%', bold)\n",
    "    \n",
    "    worksheet.write(offsetContOrDesc, 0, 'Cont_Disc')\n",
    "    worksheet.write(offsetNotes, 0, 'Notes')\n",
    "    \n",
    "\n",
    "    for column in df:\n",
    "        worksheet.write(rowIteration, colIteration, df[column].name, bold) # header\n",
    "        worksheet.write(offSetDataType, colIteration, str(df[column].dtypes)) # data type\n",
    "\n",
    "        # TODO: # of na's \n",
    "        count_nan = len(df[column]) - df[column].count()\n",
    "        \n",
    "        if count_nan > 0:\n",
    "            count_nan = str(round(( (len(df) - df[column].count()) / len(df[column]))*100, 0)) + '% - ' + str(count_nan)  \n",
    "\n",
    "\n",
    "        worksheet.write(offSetNumNaN, colIteration, count_nan) # data type\n",
    "        worksheet.write(offsetContOrDesc, colIteration, 'continous') # auto set continous - discrete determined below\n",
    "\n",
    "        # This was the original stats area.  Possibly take from the existing data set, or move around / re-factor\n",
    "        # if the columns is a numerical data type - give the summary stat\n",
    "        if np.issubdtype(df[column].dtype, np.number):\n",
    "        \n",
    "            # count\n",
    "            worksheet.write(offsetCount, colIteration, df[column].count())\n",
    "            # mean\n",
    "            worksheet.write(offsetMean, colIteration, df[column].mean())\n",
    "            # median\n",
    "            worksheet.write(offsetMedian, colIteration, df[column].median())\n",
    "            # Std\n",
    "            worksheet.write(offsetStd, colIteration, df[column].std())\n",
    "            # Var\n",
    "            worksheet.write(offsetVar, colIteration, df[column].var())\n",
    "            # Range    \n",
    "            worksheet.write(offsetRange, colIteration, df[column].max() - df[column].min())\n",
    "            # min\n",
    "            worksheet.write(offset0Prct, colIteration, df[column].quantile([0.0]))\n",
    "            # 25%\n",
    "            worksheet.write(offset25Prct, colIteration, df[column].quantile([0.25]))\n",
    "            # 50%\n",
    "            worksheet.write(offset50Prct, colIteration, df[column].quantile([0.50]))\n",
    "            # 75%\n",
    "            worksheet.write(offset75Prct, colIteration, df[column].quantile([0.75]))\n",
    "            # max\n",
    "            worksheet.write(offset100Prct, colIteration, df[column].quantile([1.0]))\n",
    "        \n",
    "        # header for the DOV\n",
    "        worksheet.write(rowDataHeader-1, colIteration, 'DOV', underline)\n",
    "\n",
    "        if df[column].nunique() > 500:\n",
    "            worksheet.write(rowDataHeader, colIteration, '> 500 unq', italic)\n",
    "            worksheet.write_column(rowDataHeader+1, colIteration, df[column].head(500))\n",
    "            colIteration = colIteration + 1\n",
    "            \n",
    "        else:\n",
    "\n",
    "            # here just saying if less than 25, then categorical vs continous - to move the dial\n",
    "            if df[column].nunique() < 100:\n",
    "                if np.issubdtype(df[column].dtype, np.number):\n",
    "                    worksheet.write(rowIteration+offsetContOrDesc, colIteration, 'discrete') # data type\n",
    "                else:\n",
    "                    worksheet.write(rowIteration+offsetContOrDesc, colIteration, 'categorical') # data type\n",
    "            \n",
    "            # grab the distribution percentage\n",
    "            disbDF = pd.DataFrame(df.groupby([column]).size() * 100 / len(df))\n",
    "            disbDF.rename(columns={0:'distprc'}, inplace=True)\n",
    "            disbDF = disbDF.sort_values(['distprc'], ascending=False)\n",
    "\n",
    "            # write the distribution header + percentage\n",
    "\n",
    "            # TODO: change the index name to \"DOV\" for better readability\n",
    "            worksheet.write_column(rowDataHeader, colIteration, disbDF.index) # here we are adding the index which is the DOV\n",
    "            colIteration = colIteration + 1\n",
    "\n",
    "            worksheet.write(rowDataHeader-1, colIteration, 'DistPrc')\n",
    "            worksheet.write_column(rowDataHeader, colIteration, disbDF.loc[: ,'distprc'])\n",
    "            colIteration = colIteration + 1\n",
    "    \n",
    "        # Notes - here give a section for the note.\n",
    "            # if all nulls, remove\n",
    "            # if two parts of the distribution are above 10% - ok, if two are about 15% even better\n",
    "            # if more than 90%, 95%, or 97% of all data is in a single category.\n",
    "\n",
    "    worksheet.freeze_panes(offsetRowDataHeader, 1) # # Freeze the first row and column\n",
    "        \n",
    "            \n",
    "    # Summary Stats - working on re-factoring this into a single call.  For now this will pivot the data for ordering\n",
    "    dfNewStat = pullSummaryStats_df(df)\n",
    "    worksheet = workbook.add_worksheet('OrderSummaryStats')\n",
    "    excel_add_df(inputWorkBook=workbook, inputWorkSheet=worksheet, inputSheetName='OrderSummaryStats', \n",
    "                 inputDataFrame=dfNewStat,inputStartRow=0,inputStartCol=0,inputUseIndex=True)\n",
    "    \n",
    "\n",
    "    # Write out the correlation matrix work-sheet\n",
    "    worksheet = workbook.add_worksheet('Correlation')\n",
    "    dfCorr = df.corr().round(2)\n",
    "    excel_add_df(inputWorkBook=workbook, inputWorkSheet=worksheet, inputSheetName='Correlation', \n",
    "                 inputDataFrame=dfCorr,inputStartRow=0,inputStartCol=0,inputUseIndex=True)\n",
    "\n",
    "    # add conditional highlighting\n",
    "    # length of columns\n",
    "    lenColsDfCorr = len(dfCorr.columns) + 1\n",
    "    lenRowsDfCorr = len(dfCorr.index) + 4 # for header\n",
    "    cellStart = xl_rowcol_to_cell(3, 2)  # C2\n",
    "    cellEnd = xl_rowcol_to_cell(len(dfCorr.index) + 2, len(dfCorr.columns) )  # C2\n",
    "    worksheet.conditional_format(cellStart + ':' + cellEnd, {'type':     'cell',\n",
    "                                            'criteria': '>=',\n",
    "                                            'value':    0.75,\n",
    "                                            'format':   formatHigh})\n",
    "    worksheet.conditional_format('B3:K12', {'type':     'cell',\n",
    "                                            'criteria': '<',\n",
    "                                            'value':    -0.75,\n",
    "                                            'format':   formatLow})\n",
    "\n",
    "    \n",
    "    # Write out the co-variance matrix work-sheet\n",
    "    worksheet = workbook.add_worksheet('Co-Variance')\n",
    "    excel_add_df(inputWorkBook=workbook, inputWorkSheet=worksheet, inputSheetName='Co-Variance', \n",
    "                 inputDataFrame=df.cov().round(2),inputStartRow=0,inputStartCol=0,inputUseIndex=True)\n",
    "\n",
    "\n",
    "    # Write out the 200 Samples - 100 head / 100 tail\n",
    "    worksheet = workbook.add_worksheet('150samples')\n",
    "    excel_add_df(inputWorkBook=workbook, inputWorkSheet=worksheet, inputSheetName='150samples (Top 75)', \n",
    "                 inputDataFrame=df.head(75),inputStartRow=0,inputStartCol=0,inputUseIndex=True)\n",
    "    \n",
    "    excel_add_df(inputWorkBook=workbook, inputWorkSheet=worksheet, inputSheetName='150samples (Tail 75)', \n",
    "                 inputDataFrame=df.tail(75),inputStartRow=80,inputStartCol=0,inputUseIndex=True)\n",
    "\n",
    "    # auto-plotting will go here\n",
    "    \n",
    "    # close wb\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This actually runs all of the functions, reads the files, and outputs the files\n",
    "# The file open portion is designed to work on a Mac, other OS systems may differ\n",
    "\n",
    "import subprocess, os\n",
    "\n",
    "# Read the input file\n",
    "df = pd.read_csv(inputFileNme)\n",
    "\n",
    "# Generate the excel file\n",
    "excel_gen_file(df, outputFileName)\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "# Open the file\n",
    "os.system(\"open \"+outputFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# PERSONAL NOTES LOG \n",
    "####################\n",
    "\n",
    "# NEXT STEPS\n",
    "# - for the correlation.  Put anything above .50 in an orange / light blue status.  Then anything more in deep red/blue\n",
    "# - Put a notes section for whether to keep or not. \n",
    "# - Include a possibility for a list of columns to go in to keep.\n",
    "# - If you have classifiers / Drivers, then show distribution against that (if a binary classification) - maybe save for plotting script?\n",
    "# - add something that will indicate if there are major groupings (by percentage) - mainly want to see if groups are differnt\n",
    "# - reg expression items\n",
    "# - something that detects years (1900 - 2000), the word year or word month (with 75% success)\n",
    "# - Min/Max on strings / dates\n",
    "# - ?Insert Plots Images? - worksheet.insert_image('B2', 'python.png')\n",
    "\n",
    "# INTELLIGENT NOTES\n",
    "# -- mark > 25%, 40%, 70%, 90% nulls\n",
    "\n",
    "# BUGS TO FIX\n",
    "# -- In the ordered summary stats tab, clear out the extra line\n",
    "\n",
    "# COMPLETED ITEMS\n",
    "# X - Add conditional formatting to correlation to see highly correlated or negative correlated\n",
    "# X - Need to add a variance to the stats summary data frame.  \n",
    "# X - Identify the continous, vs descrete\n",
    "# X - for n/a it is calculating percentage incorrectly - see mths_since_last_delinq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
